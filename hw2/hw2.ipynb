{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.agafonov/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import scipy.integrate as integrate\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.colors import LogNorm\n",
    "import pickle\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import cProfile\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "palette = sns.color_palette()\n",
    "figsize = (15,8)\n",
    "legend_fontsize = 16\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif'})\n",
    "# rc('text', usetex=True)\n",
    "# rc('text.latex',preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "# rc('text.latex',preamble=r'\\usepackage[russian]{babel}')\n",
    "rc('figure', **{'dpi': 300})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS, N_COLS, N_WIN = 3, 3, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe(gym.Env):\n",
    "    def __init__(self, n_rows=N_ROWS, n_cols=N_COLS, n_win=N_WIN, clone=None):\n",
    "        if clone is not None:\n",
    "            self.n_rows, self.n_cols, self.n_win = clone.n_rows, clone.n_cols, clone.n_win\n",
    "            self.board = copy.deepcopy(clone.board)\n",
    "            self.curTurn = clone.curTurn\n",
    "            self.emptySpaces = None\n",
    "            self.boardHash = None\n",
    "        else:\n",
    "            self.n_rows = n_rows\n",
    "            self.n_cols = n_cols\n",
    "            self.n_win = n_win\n",
    "\n",
    "            self.reset()\n",
    "\n",
    "    def getEmptySpaces(self):\n",
    "        if self.emptySpaces is None:\n",
    "            res = np.where(self.board == 0)\n",
    "            self.emptySpaces = np.array([ (i, j) for i,j in zip(res[0], res[1]) ])\n",
    "        return self.emptySpaces\n",
    "\n",
    "    def makeMove(self, player, i, j):\n",
    "        self.board[i, j] = player\n",
    "        self.emptySpaces = None\n",
    "        self.boardHash = None\n",
    "\n",
    "    def getHash(self):\n",
    "        if self.boardHash is None:\n",
    "            self.boardHash = ''.join(['%s' % (x+1) for x in self.board.reshape(self.n_rows * self.n_cols)])\n",
    "        return self.boardHash\n",
    "\n",
    "    def isTerminal(self):\n",
    "        # проверим, не закончилась ли игра\n",
    "        cur_marks, cur_p = np.where(self.board == self.curTurn), self.curTurn\n",
    "        for i,j in zip(cur_marks[0], cur_marks[1]):\n",
    "            win = False\n",
    "            if i <= self.n_rows - self.n_win:\n",
    "                if np.all(self.board[i:i+self.n_win, j] == cur_p):\n",
    "                    win = True\n",
    "            if not win:\n",
    "                if j <= self.n_cols - self.n_win:\n",
    "                    if np.all(self.board[i,j:j+self.n_win] == cur_p):\n",
    "                        win = True\n",
    "            if not win:\n",
    "                if i <= self.n_rows - self.n_win and j <= self.n_cols - self.n_win:\n",
    "                    if np.all(np.array([ self.board[i+k,j+k] == cur_p for k in range(self.n_win) ])):\n",
    "                        win = True\n",
    "            if not win:\n",
    "                if i <= self.n_rows - self.n_win and j >= self.n_win-1:\n",
    "                    if np.all(np.array([ self.board[i+k,j-k] == cur_p for k in range(self.n_win) ])):\n",
    "                        win = True\n",
    "            if win:\n",
    "                self.gameOver = True\n",
    "                return self.curTurn\n",
    "\n",
    "        if len(self.getEmptySpaces()) == 0:\n",
    "            self.gameOver = True\n",
    "            return 0\n",
    "\n",
    "        self.gameOver = False\n",
    "        return None\n",
    "\n",
    "    def printBoard(self):\n",
    "        for i in range(0, self.n_rows):\n",
    "            print('----'*(self.n_cols)+'-')\n",
    "            out = '| '\n",
    "            for j in range(0, self.n_cols):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('----'*(self.n_cols)+'-')\n",
    "\n",
    "    def getState(self):\n",
    "        return (self.getHash(), self.getEmptySpaces(), self.curTurn)\n",
    "\n",
    "    def action_from_int(self, action_int):\n",
    "        return ( int(action_int / self.n_cols), int(action_int % self.n_cols))\n",
    "\n",
    "    def int_from_action(self, action):\n",
    "        return action[0] * self.n_cols + action[1]\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.board[action[0], action[1]] != 0:\n",
    "            return self.getState(), -10, True, {}\n",
    "        self.makeMove(self.curTurn, action[0], action[1])\n",
    "        reward = self.isTerminal()\n",
    "        self.curTurn = -self.curTurn\n",
    "        return self.getState(), 0 if reward is None else reward, reward is not None, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.n_rows, self.n_cols), dtype=int)\n",
    "        self.boardHash = None\n",
    "        self.gameOver = False\n",
    "        self.emptySpaces = None\n",
    "        self.curTurn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_board(env, pi, showtext=True, verbose=True, fontq=20, fontx=60):\n",
    "    '''Рисуем доску с оценками из стратегии pi'''\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    X, Y = np.meshgrid(np.arange(0, env.n_rows), np.arange(0, env.n_rows))\n",
    "    Z = np.zeros((env.n_rows, env.n_cols)) + .01\n",
    "    s, actions = env.getHash(), env.getEmptySpaces()\n",
    "    if pi is not None and s in pi.Q:\n",
    "        for i, a in enumerate(actions):\n",
    "            Z[a[0], a[1]] = pi.Q[s][i]\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    surf = ax.imshow(Z, cmap=plt.get_cmap('Accent', 10), vmin=-1, vmax=1)\n",
    "    if showtext:\n",
    "        for i,a in enumerate(actions):\n",
    "            if pi is not None and s in pi.Q:\n",
    "                ax.text( a[1] , a[0] , \"%.3f\" % pi.Q[s][i], fontsize=fontq, horizontalalignment='center', verticalalignment='center', color=\"w\" )\n",
    "    for i in range(env.n_rows):\n",
    "        for j in range(env.n_cols):\n",
    "            if env.board[i, j] == -1:\n",
    "                ax.text(j, i, \"O\", fontsize=fontx, horizontalalignment='center', verticalalignment='center', color=\"w\" )\n",
    "            if env.board[i, j] == 1:\n",
    "                ax.text(j, i, \"X\", fontsize=fontx, horizontalalignment='center', verticalalignment='center', color=\"w\" )\n",
    "#     cbar = plt.colorbar(surf, ticks=[0, 1])\n",
    "    ax.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "def get_and_print_move(env, pi, s, actions, random=False, verbose=True, fontq=20, fontx=60):\n",
    "    '''Делаем ход, рисуем доску'''\n",
    "    plot_board(env, pi, fontq=fontq, fontx=fontx)\n",
    "    if verbose and (pi is not None):\n",
    "        if s in pi.Q:\n",
    "            for i,a in enumerate(actions):\n",
    "                print(i, a, pi.Q[s][i])\n",
    "        else:\n",
    "            print(\"Стратегия не знает, что делать...\")\n",
    "    if random:\n",
    "        return np.random.randint(len(actions))\n",
    "    else:\n",
    "        return pi.getActionGreedy(s, len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_game(env, pi1, pi2, random_crosses=False, random_naughts=True, verbose=True, fontq=20, fontx=60):\n",
    "    '''Играем тестовую партию между стратегиями или со случайными ходами, рисуем ход игры'''\n",
    "    done = False\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        s, actions = env.getHash(), env.getEmptySpaces()\n",
    "        if env.curTurn == 1:\n",
    "            a = get_and_print_move(env, pi1, s, actions, random=random_crosses, verbose=verbose, fontq=fontq, fontx=fontx)\n",
    "        else:\n",
    "            a = get_and_print_move(env, pi2, s, actions, random=random_naughts, verbose=verbose, fontq=fontq, fontx=fontx)\n",
    "        observation, reward, done, info = env.step(actions[a])\n",
    "        if reward == 1:\n",
    "            print(\"Крестики выиграли!\")\n",
    "            plot_board(env, None, showtext=False, fontq=fontq, fontx=fontx)\n",
    "        if reward == -1:\n",
    "            print(\"Нолики выиграли!\")\n",
    "            plot_board(env, None, showtext=False, fontq=fontq, fontx=fontx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.env.reset()\n",
    "        self.actions = list(range(len(env.getState()[1]))) \n",
    "        self.q_table = defaultdict(lambda: np.zeros(9)) \n",
    "        \n",
    "            \n",
    "    def choose_action(self, state, epsilon):\n",
    "        impossible_positions = np.where(np.array(list(state)) != \"1\")[0]\n",
    "        possible_positions = np.where(np.array(list(state)) == \"1\")[0]\n",
    "        for k in impossible_positions:\n",
    "                self.q_table[state][k] = -np.inf\n",
    "        action = np.argmax(self.q_table[state])       \n",
    "                \n",
    "        action = (np.random.choice(possible_positions) if epsilon > random.uniform(0, 1) else action)\n",
    "        return action\n",
    "        \n",
    "           \n",
    "    def training(self, n_iterations, epsilon=0.5, alpha=0.01, gamma=0.99):\n",
    "        for i in range(n_iterations):\n",
    "            self.env.reset()\n",
    "            state_cross = self.env.getHash()\n",
    "            action_cross = self.choose_action(state_cross, epsilon)\n",
    "            (state_nought, _, turn), reward, done, info = self.env.step(self.env.action_from_int(action_cross))\n",
    "            action_nought = self.choose_action(state_nought, epsilon)\n",
    "            (state_cross, _, turn), reward, done, info = self.env.step(self.env.action_from_int(action_nought))\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                \n",
    "                if self.env.curTurn == 1:            \n",
    "                    action_cross = self.choose_action(state_cross, epsilon)\n",
    "                    (state_nought_new, _, turn), reward, done, info = self.env.step(\n",
    "                        self.env.action_from_int(action_cross))\n",
    "                    if reward == 1:\n",
    "                        self.q_table[state_cross][action_cross] == 1\n",
    "                    \n",
    "                    update_q_nought = alpha * (-1 * reward  + gamma * np.max(self.q_table[state_nought_new]) - self.q_table[state_nought][action_nought])\n",
    "                    self.q_table[state_nought][action_nought] += update_q_nought\n",
    "                    \n",
    "                    state_nought = state_nought_new\n",
    "                else:\n",
    "                    action_nought = self.choose_action(state_nought, epsilon)\n",
    "                    (state_cross_new, _, turn), reward, done, info = self.env.step(\n",
    "                        self.env.action_from_int(action_nought))\n",
    "        \n",
    "                    if reward == -1:\n",
    "                        self.q_table[state_nought][action_nought] == 1\n",
    "                    \n",
    "                    if reward == 10:\n",
    "                        self.q_table[state_nought][action_nought] == -10\n",
    "                    \n",
    "                    update_q_cross = alpha * (1 * reward  + gamma * np.max(self.q_table[state_cross_new]) - self.q_table[state_cross][action_cross])\n",
    "                    self.q_table[state_cross][action_cross] += update_q_cross\n",
    "                    \n",
    "                    state_cross = state_cross_new\n",
    "\n",
    "                    \n",
    "    def play(self, player):\n",
    "        self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state = self.env.getHash()\n",
    "            if self.env.curTurn == player:\n",
    "                action = np.argmax(self.q_table[state])\n",
    "            else:\n",
    "                action = self.choose_action(state, epsilon=1)\n",
    "            (state, _, _), reward, done, info = self.env.step(self.env.action_from_int(action))\n",
    "        return reward * player\n",
    "    \n",
    "    \n",
    "    def evaluate(self, player, n_iter):\n",
    "        player_reward = []\n",
    "        for i in range(n_iter):\n",
    "            reward = self.play(player)\n",
    "            if abs(reward) != 10:\n",
    "                player_reward.append(reward)\n",
    "        return(player_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql = QLearning(TicTacToe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50000 iterations\n",
      "   Win rate for noughts: 0.607\n",
      "   Win rate for crosses: 0.939\n",
      "After 100000 iterations\n",
      "   Win rate for noughts: 0.69\n",
      "   Win rate for crosses: 0.927\n",
      "After 150000 iterations\n",
      "   Win rate for noughts: 0.71\n",
      "   Win rate for crosses: 0.935\n",
      "After 200000 iterations\n",
      "   Win rate for noughts: 0.723\n",
      "   Win rate for crosses: 0.938\n",
      "After 250000 iterations\n",
      "   Win rate for noughts: 0.686\n",
      "   Win rate for crosses: 0.921\n",
      "After 300000 iterations\n",
      "   Win rate for noughts: 0.702\n",
      "   Win rate for crosses: 0.945\n"
     ]
    }
   ],
   "source": [
    "crosses = []\n",
    "noughts = []\n",
    "for i in list(range(1, 7)):\n",
    "    ql.training(50000)\n",
    "    print(f\"After {50000 * i} iterations\")\n",
    "    n_iter = 1000\n",
    "    crosses.append(np.mean(ql.evaluate(-1, n_iter)))\n",
    "    noughts.append(np.mean(ql.evaluate(1, n_iter)))\n",
    "    print(f\"   Win rate for noughts: {np.mean(ql.evaluate(-1, n_iter))}\")\n",
    "    print(f\"   Win rate for crosses: {np.mean(ql.evaluate(1, n_iter))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
